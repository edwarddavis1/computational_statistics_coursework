results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="blue") +
geom_abline(intercept=-new_new_new_w[3]/new_new_new_w[2], slope=-new_new_new_w[1]/new_new_new_w[2], colour="red") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = 0.5)
new_new_w = logistic_regression(input_data, y, condition_value = 0.001)
new_new_new_w = logistic_regression(input_data, y, condition_value = 0.99)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="blue") +
geom_abline(intercept=-new_new_new_w[3]/new_new_new_w[2], slope=-new_new_new_w[1]/new_new_new_w[2], colour="red") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = 0.5)
new_new_w = logistic_regression(input_data, y, condition_value = 0.001)
new_new_new_w = logistic_regression(input_data, y, condition_value = 0.95)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="blue") +
geom_abline(intercept=-new_new_new_w[3]/new_new_new_w[2], slope=-new_new_new_w[1]/new_new_new_w[2], colour="red") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = 0.5)
new_new_w = logistic_regression(input_data, y, condition_value = 0.001)
new_new_new_w = logistic_regression(input_data, y, condition_value = 0.93)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="blue") +
geom_abline(intercept=-new_new_new_w[3]/new_new_new_w[2], slope=-new_new_new_w[1]/new_new_new_w[2], colour="red") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = 0.5)
new_new_w = logistic_regression(input_data, y, condition_value = 0.001)
new_new_new_w = logistic_regression(input_data, y, condition_value = 0.93)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="blue") +
geom_abline(intercept=-new_new_new_w[3]/new_new_new_w[2], slope=-new_new_new_w[1]/new_new_new_w[2], colour="red") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
sigmoid = function(x, bias=0.5) return(1/(exp(bias * x)+1))
logistic_regression <- function(input_data, y, max_iter=100, lr_0=10, condition_value=0.5) {
# Random initialisation of weights
# Calculate f(x; w)
x = as.matrix(input_data)
n = dim(x)[1]
d = dim(x)[2]
ones = rep(1, n)
X = as.matrix(rbind(t(x), ones))
w = as.matrix(runif(d+1, min=-1, 1), ncol=1)
f = t(w) %*% X
for (j in 1:max_iter) {
# Learning Rate
lr = lr_0 / j
for (i in 1:n) {
f = t(w) %*% X[,i]
if (sigmoid(f * y[i]) >= condition_value) {
w = w + lr * y[i] * X[,i]
}
}
}
return (w)
}
sigmoid = function(x, bias=0.5) return(1/(exp(bias * x)+1))
logistic_regression <- function(input_data, y, max_iter=100, lr_0=10, condition_value=0.5) {
# Random initialisation of weights
# Calculate f(x; w)
x = as.matrix(input_data)
n = dim(x)[1]
d = dim(x)[2]
ones = rep(1, n)
X = as.matrix(rbind(t(x), ones))
w = as.matrix(runif(d+1, min=-1, 1), ncol=1)
f = t(w) %*% X
for (j in 1:max_iter) {
# Learning Rate
lr = lr_0 / j
for (i in 1:n) {
f = t(w) %*% X[,i]
if (sigmoid(f * y[i], bias=condition_value) >= condition_value) {
w = w + lr * y[i] * X[,i]
}
}
}
return (w)
}
input_cols = c(1,2)
input_data = train_data[,input_cols]
y = as.matrix(train_data$y)
my_w = logistic_regression(input_data, y)
## Results
```{r}
results_plot = ggplot(train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
```
## Result biases
```{r}
false_positives = 0
false_negatives = 0
for (i in 1:dim(train_data)[1]) {
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
results_plot = ggplot(train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_data = train_data[,input_cols]
y = as.matrix(train_data$y)
my_w = logistic_regression(input_data, y)
results_plot = ggplot(train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
results_plot = ggplot(train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
false_positives = 0
false_negatives = 0
for (i in 1:dim(train_data)[1]) {
# y = -1 classed as y = 1 == False positive
if ((train_data$x_2[i] > -my_w[3]/my_w[2] -my_w[1]/my_w[2] * train_data$x_1[i]) & train_data$y[i] == -1) {
false_positives = false_positives + 1
}
# y = 1 classed as y = -1 == False negative
if ((train_data$x_2[i] < -my_w[3]/my_w[2] -my_w[1]/my_w[2] * train_data$x_1[i]) & train_data$y[i] == 1) {
false_negatives = false_negatives + 1
}
}
false_positives
false_negatives
false_positives
# Distribution 1 (category 1)
samples_1 = 2000
means = c(2,1)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mvn_1 <- mvn_dist(means, sigma, samples_1)
# Distribution 2 (category 2)
samples_2 = 100
means = c(2,4)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mvn_2 <- mvn_dist(means, sigma, samples_2)
ratio = samples_1 / samples_2
# Separated data
separated_data = data.frame(
"Cat1x"=mvn_1[1,],
"Cat1y"=mvn_1[2,],
"Cat2x"=mvn_2[1,],
"Cat2y"=mvn_2[2,]
)
# Un-separated training data
train_x = append(mvn_1[1,], mvn_2[1,])
train_y = append(mvn_1[2,], mvn_2[2,])
train_data_unshuffled = data.frame(
x_1=train_x,
x_2=train_y,
y=append(rep(-1, length(mvn_1[1,])),
rep(1, length(mvn_2[1,])))
)
# Shuffle data
set.seed(123)
shuffled_rows = sample(nrow(train_data_unshuffled))
new_train_data = train_data_unshuffled[shuffled_rows,]
# Plot of separated data
initial_data_plot <- ggplot(separated_data, aes(x_1,x_2))
initial_data_plot +
geom_point(data=new_train_data, aes(x_1,x_2, colour=y))
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = ratio)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
new_false_positives = 0
new_false_negatives = 0
for (i in 1:dim(new_train_data)[1]) {
# y = -1 classed as y = 1 == False positive
if ((new_train_data$x_2[i] > -my_w[3]/my_w[2] -my_w[1]/my_w[2] * new_train_data$x_1[i]) & new_train_data$y[i] == -1) {
new_false_positives = new_false_positives + 1
}
# y = 1 classed as y = -1 == False negative
if ((new_train_data$x_2[i] < -my_w[3]/my_w[2] -my_w[1]/my_w[2] * new_train_data$x_1[i]) & new_train_data$y[i] == 1) {
new_false_negatives = new_false_negatives + 1
}
}
new_false_positives
new_false_negatives
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = ratio)
new_new_w = logistic_regression(input_data, y, condition_value = 0.5)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="pink") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = ratio)
new_new_w = logistic_regression(input_data, y, condition_value = 0.5)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="pink") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = ratio*2)
new_new_w = logistic_regression(input_data, y, condition_value = 1)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="pink") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = ratio*2)
new_new_w = logistic_regression(input_data, y, condition_value = 1)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
# geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="pink") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
knitr::opts_chunk$set(echo = TRUE)
mvn_dist <- function(means, sigma, n) {
# Eigen-decomposition
sigma_inv = solve(sigma)
sigma_inv_eigens = eigen(sigma_inv)
U = sigma_inv_eigens$vectors
d <- sigma_inv_eigens$values
D = solve(U) %*% sigma_inv %*% U
# Multivariate normal sample matrix
x = matrix(nrow=dim(sigma)[1], ncol=n)
for (j in 1:n) {
y <- c()
for (i in 1:dim(sigma)[1]) {
# Sample univariate normal
y[i] <- rnorm(1, mean=0, sd=sqrt(1/d[i]))
}
# Rearrangement of y substitution
x_sample <- (U %*% y) + means
# Appends samples for each column for ith row
x[,j] <- x_sample
}
return(x)
}
# Distribution 1 (category 1)
samples = 2000
means = c(2,1)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mvn_1 <- mvn_dist(means, sigma, samples)
# Distribution 2 (category 2)
means = c(2,4)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mvn_2 <- mvn_dist(means, sigma, samples)
# Separated data
separated_data = data.frame(
"Cat1x"=mvn_1[1,],
"Cat1y"=mvn_1[2,],
"Cat2x"=mvn_2[1,],
"Cat2y"=mvn_2[2,]
)
# Un-separated training data
train_x = append(mvn_1[1,], mvn_2[1,])
train_y = append(mvn_1[2,], mvn_2[2,])
train_data_unshuffled = data.frame(
x_1=train_x,
x_2=train_y,
y=append(rep(-1, length(mvn_1[1,])),
rep(1, length(mvn_1[1,])))
)
# Shuffle data
set.seed(123)
shuffled_rows = sample(nrow(train_data_unshuffled))
train_data = train_data_unshuffled[shuffled_rows,]
# Plot of separated data
library(ggplot2)
initial_data_plot <- ggplot(separated_data, aes(x_1,x_2))
initial_data_plot +
geom_point(data=train_data, aes(x_1,x_2, colour=y))
sigmoid = function(x, bias=0.5) return(1/(exp(bias * x)+1))
logistic_regression <- function(input_data, y, max_iter=100, lr_0=10, condition_value=0.5) {
# Random initialisation of weights
# Calculate f(x; w)
x = as.matrix(input_data)
n = dim(x)[1]
d = dim(x)[2]
ones = rep(1, n)
X = as.matrix(rbind(t(x), ones))
w = as.matrix(runif(d+1, min=-1, 1), ncol=1)
f = t(w) %*% X
for (j in 1:max_iter) {
# Learning Rate
lr = lr_0 / j
for (i in 1:n) {
f = t(w) %*% X[,i]
if (sigmoid(f * y[i], bias=condition_value) >= condition_value) {
w = w + lr * y[i] * X[,i]
}
}
}
return (w)
}
input_cols = c(1,2)
input_data = train_data[,input_cols]
y = as.matrix(train_data$y)
my_w = logistic_regression(input_data, y)
results_plot = ggplot(train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
false_positives = 0
false_negatives = 0
for (i in 1:dim(train_data)[1]) {
# y = -1 classed as y = 1 == False positive
if ((train_data$x_2[i] > -my_w[3]/my_w[2] -my_w[1]/my_w[2] * train_data$x_1[i]) & train_data$y[i] == -1) {
false_positives = false_positives + 1
}
# y = 1 classed as y = -1 == False negative
if ((train_data$x_2[i] < -my_w[3]/my_w[2] -my_w[1]/my_w[2] * train_data$x_1[i]) & train_data$y[i] == 1) {
false_negatives = false_negatives + 1
}
}
false_positives
false_negatives
# Distribution 1 (category 1)
samples_1 = 2000
means = c(2,1)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mvn_1 <- mvn_dist(means, sigma, samples_1)
# Distribution 2 (category 2)
samples_2 = 100
means = c(2,4)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mvn_2 <- mvn_dist(means, sigma, samples_2)
ratio = samples_1 / samples_2
# Separated data
separated_data = data.frame(
"Cat1x"=mvn_1[1,],
"Cat1y"=mvn_1[2,],
"Cat2x"=mvn_2[1,],
"Cat2y"=mvn_2[2,]
)
# Un-separated training data
train_x = append(mvn_1[1,], mvn_2[1,])
train_y = append(mvn_1[2,], mvn_2[2,])
train_data_unshuffled = data.frame(
x_1=train_x,
x_2=train_y,
y=append(rep(-1, length(mvn_1[1,])),
rep(1, length(mvn_2[1,])))
)
# Shuffle data
set.seed(123)
shuffled_rows = sample(nrow(train_data_unshuffled))
new_train_data = train_data_unshuffled[shuffled_rows,]
# Plot of separated data
initial_data_plot <- ggplot(separated_data, aes(x_1,x_2))
initial_data_plot +
geom_point(data=new_train_data, aes(x_1,x_2, colour=y))
# input_cols = c(1,2)
# input_data = new_train_data[,input_cols]
# y = as.matrix(new_train_data$y)
#
# new_w = logistic_regression(input_data, y)
input_cols = c(1,2)
input_data = new_train_data[,input_cols]
y = as.matrix(new_train_data$y)
new_w = logistic_regression(input_data, y, condition_value = ratio*2)
new_new_w = logistic_regression(input_data, y, condition_value = 1)
results_plot = ggplot(new_train_data, aes(x_1, x_2))
results_plot +
geom_point(aes(x_1, x_2, colour=factor(y)),alpha=0.5) +
geom_abline(intercept=-new_w[3]/new_w[2], slope=-new_w[1]/new_w[2], colour="orchid") +
geom_abline(intercept=-new_new_w[3]/new_new_w[2], slope=-new_new_w[1]/new_new_w[2], colour="pink") +
geom_abline(intercept=-my_w[3]/my_w[2], slope=-my_w[1]/my_w[2], colour="steelblue")
outR = 11.5
inR = 2.75
width = 1.8
vol = pi(11.5^2 + 2.75^2) * width
vol
knitr::opts_chunk$set(echo = TRUE)
outR = 11.5
inR = 2.75
width = 1.8
vol = pi(11.5^2 + 2.75^2) * width
vol
vol = pi(11.5^2 + 2.75^2) * width
vol = pi * (11.5^2 + 2.75^2) * width
vol
h = 0.2
l = vol / (h * width)
l
l / 1000
outR_E = 0.1
inR_E = 0.1
width_E = 0.1
vol
l * h * width
vol
vol = pi * (11.5^2 - 2.75^2) * width
vol
h = 0.2
l = vol / (h * width)
l / 1000
install.packages("keras")
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
system("ls *.c")
setwd("~/Documents/PhD_TB2/comp/comp_week_2")
system("ls *.c")
system("R CMD SHLIB expSmooth.c")
dyn.load("expSmooth.so")
system("R CMD SHLIB expSmooth.c")
system("R CMD SHLIB expSmooth.c")
install.packages("Rcpp")
system("ls *.c")
system("R CMD SHLIB expSmooth.c")
system("ls *.c")
system("R CMD SHLIB expSmooth.c")
dyn.load("expSmooth.so")
is.loaded("expsmooth")
is.loaded("expSmooth")
is.loaded("expSmooth")
library(gamair)
install.packages('gamair')
library(gamair)
library(tibble)
data(chicago)
chicago = as_tibble(chicago)
chicago
nch = nrow(chicago)
tmpSmooth = numeric(nch)
.Call("expSmooth", chicago$tmpd, tmpSmooth, nch, 0.5)
plot(chicago$tmpd[1:1000], col = "grey", ylab = "Temp")
plot(chicago$tmpd[1:1000], col = "grey", ylab = "Temp")
lines(tmpSmooth[1:1000], col = 2)
library(gamair)
library(tibble)
data(chicago)
chicago = as_tibble(chicago)
nch = nrow(chicago)
tmpSmooth = numeric(nch)
library(gamair)
library(tibble)
data(chicago)
chicago = as_tibble(chicago)
nch = nrow(chicago)
tmpSmooth = numeric(nch)
tmpSmooth
system("ls *.c")
system("R CMD SHLIB expSmooth.c")
dyn.load("expSmooth.so")
is.loaded("expSmooth")
.Call("expSmooth", chicago$tmpd, tmpSmooth, nch, 0.5)
tmpSmooth
system("ls *.c")
system("R CMD SHLIB rickerSimul.C")
dyn.load("rickerSimul.so")
system("R CMD SHLIB rickerSimul.C")
dyn.load("rickerSimul.so")
system("ls *.c")
system("R CMD SHLIB rickerSimul.c")
system("ls *.c")
system("R CMD SHLIB rickerSimul.c")
system("ls *.c")
system("R CMD SHLIB rickerSimul.c")
system("R CMD SHLIB rickerSimul.c")
system("R CMD SHLIB rickerSimul.c")
system("ls *.c")
system("R CMD SHLIB rickerSimul.c")
system("ls *.c")
system("R CMD SHLIB rickerSimul.c")
